{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from simpletransformers.language_modeling import LanguageModelingModel\n",
    "from simpletransformers.language_generation import LanguageGenerationModel\n",
    "import warnings\n",
    "import pickle\n",
    "import re\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(post):\n",
    "    post_split = post.split('|||')\n",
    "    # split the kaggle data set posts by |||\n",
    "    post_split_split = [x.split(' ') for x in post_split]\n",
    "    \n",
    "    # removes any 'words' that have http:// or https:// in them\n",
    "    return_list = [[item for item in sentence if ('http://' not in item and 'https://' not in item)] for sentence in post_split_split]\n",
    "    \n",
    "    # returns a list of posts if they are not empty after removing the links\n",
    "    return [' '.join(sentence) for sentence in return_list if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/mbti_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'INTJ':0, 'INTP':0, 'ENTJ':1, 'ENTP':1, \n",
    "           'INFJ':0, 'INFP':0, 'ENFJ':1, 'ENFP':1, \n",
    "           'ISTJ':0, 'ISFJ':0, 'ESTJ':1, 'ESFJ':1, \n",
    "           'ISTP':0, 'ISFP':0, 'ESTP':1, 'ESFP':1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTJ (1091, 2)\n",
      "INTP (1304, 2)\n",
      "ENTJ (231, 2)\n",
      "ENTP (685, 2)\n",
      "INFJ (1470, 2)\n",
      "INFP (1832, 2)\n",
      "ENFJ (190, 2)\n",
      "ENFP (675, 2)\n",
      "ISTJ (205, 2)\n",
      "ISFJ (166, 2)\n",
      "ESTJ (39, 2)\n",
      "ESFJ (42, 2)\n",
      "ISTP (337, 2)\n",
      "ISFP (271, 2)\n",
      "ESTP (89, 2)\n",
      "ESFP (48, 2)\n"
     ]
    }
   ],
   "source": [
    "for dataframe in df_dict.keys():\n",
    "    df_dict[dataframe] = df[df['type'] == dataframe]\n",
    "    print(dataframe, df_dict[dataframe].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INTJ (1091, 3)\n",
      "INTP (1304, 3)\n",
      "ENTJ (231, 3)\n",
      "ENTP (685, 3)\n",
      "INFJ (1470, 3)\n",
      "INFP (1832, 3)\n",
      "ENFJ (190, 3)\n",
      "ENFP (675, 3)\n",
      "ISTJ (205, 3)\n",
      "ISFJ (166, 3)\n",
      "ESTJ (39, 3)\n",
      "ESFJ (42, 3)\n",
      "ISTP (337, 3)\n",
      "ISFP (271, 3)\n",
      "ESTP (89, 3)\n",
      "ESFP (48, 3)\n"
     ]
    }
   ],
   "source": [
    "for dataframe in df_dict.keys():\n",
    "    df_dict[dataframe]['post_split'] = df_dict[dataframe].posts.apply(clean)\n",
    "    print(dataframe, df_dict[dataframe].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>posts</th>\n",
       "      <th>post_split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Dear INTP,   I enjoyed our conversation the o...</td>\n",
       "      <td>['Dear INTP,   I enjoyed our conversation the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'18/37 @.@|||Science  is not perfect. No scien...</td>\n",
       "      <td>['18/37 @.@, Science  is not perfect. No scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'I tend to build up a collection of things on ...</td>\n",
       "      <td>['I tend to build up a collection of things on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Fair enough, if that's how you want to look a...</td>\n",
       "      <td>['Fair enough, if that's how you want to look ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>INTJ</td>\n",
       "      <td>'Poker face for sure, accompanied by some sarc...</td>\n",
       "      <td>['Poker face for sure, accompanied by some sar...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type                                              posts  \\\n",
       "3   INTJ  'Dear INTP,   I enjoyed our conversation the o...   \n",
       "5   INTJ  '18/37 @.@|||Science  is not perfect. No scien...   \n",
       "7   INTJ  'I tend to build up a collection of things on ...   \n",
       "13  INTJ  'Fair enough, if that's how you want to look a...   \n",
       "36  INTJ  'Poker face for sure, accompanied by some sarc...   \n",
       "\n",
       "                                           post_split  \n",
       "3   ['Dear INTP,   I enjoyed our conversation the ...  \n",
       "5   ['18/37 @.@, Science  is not perfect. No scien...  \n",
       "7   ['I tend to build up a collection of things on...  \n",
       "13  ['Fair enough, if that's how you want to look ...  \n",
       "36  ['Poker face for sure, accompanied by some sar...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict['INTJ'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataframe in df_dict.keys():\n",
    "    df_dict[dataframe] = df_dict[dataframe].post_split.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for type_ in df_dict.keys():\n",
    "    with open(f\"../data/GPT-2_text_gen_posts/{type_}_posts_train.txt\", \"w\") as f:\n",
    "        for post in df_dict[type_][:-10]:\n",
    "            for ind_post in post:\n",
    "                if(re.search('[a-zA-Z]', ind_post)):\n",
    "                    f.writelines(ind_post + \"\\n\")\n",
    "\n",
    "    with open(f\"../data/GPT-2_text_gen_posts/{type_}_posts_test.txt\", \"w\") as f:\n",
    "        for post in df_dict[type_][-10:]:\n",
    "            for ind_post in post:\n",
    "                if(re.search('[a-zA-Z]', ind_post)):\n",
    "                    f.writelines(ind_post + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = {\n",
    "    \"reprocess_input_data\": True,\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"train_batch_size\": 64,\n",
    "    \"num_train_epochs\": 3,\n",
    "    \"mlm\": False,\n",
    "}\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4abe6eb9a6dc48ef89b081a8fd390eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=7377.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c374dac89d54e4b8346cdf25a86fc4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2103.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c8af501b4947b7a33d1763f46364a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Epoch'), FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c81cb3af3e8b4aae832b56f169741d1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 0 of 3'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895a845e005c4fa1b350355e9e81d488",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 1 of 3'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19d23d89fe024e098dc7603ab68fadcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Epoch 2 of 3'), FloatProgress(value=0.0, max=33.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(99, 3.9009252581933533)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type_ = 'ISFJ'\n",
    "model = LanguageModelingModel('gpt2', 'gpt2', args=train_args, use_cuda=False)\n",
    "\n",
    "model.train_model(train_file = f\"../data/GPT-2_text_gen_posts/{type_}_posts_train.txt\", \n",
    "                  eval_file = f\"../data/GPT-2_text_gen_posts/{type_}_posts_test.txt\", \n",
    "                  output_dir = f\"gen_lang_models/{type_}_lang_model/\")\n",
    "\n",
    "# pickle.dump(model, open(f'models/{type_}_lang_model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2b6a395672048fd95d84985e02b860e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=463.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e34ba165b6436dacc9566661ab077c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=115.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaf0ba9d025847d89c1e48fb4b4dfb68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Running Evaluation'), FloatProgress(value=0.0, max=15.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results.append((type_, model.eval_model(f\"../data/GPT-2_text_gen_posts/{type_}_posts_test.txt\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'gen_lang_models/ESFP_lang_model/'. Make sure that:\n\n- 'gen_lang_models/ESFP_lang_model/' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'gen_lang_models/ESFP_lang_model/' is the correct path to a directory containing relevant tokenizer files\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e887dc654be6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageGenerationModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"gen_lang_models/ESFP_lang_model/\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"max_length\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_cuda\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/lewagon/lib/python3.7/site-packages/simpletransformers/language_generation/language_generation_model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model_type, model_name, args, use_cuda, cuda_device, **kwargs)\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.7.7/envs/lewagon/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1760\u001b[0m                 \u001b[0;34mf\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing relevant tokenizer files\\n\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1761\u001b[0m             )\n\u001b[0;32m-> 1762\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1763\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1764\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mfile_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvocab_files\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: Can't load tokenizer for 'gen_lang_models/ESFP_lang_model/'. Make sure that:\n\n- 'gen_lang_models/ESFP_lang_model/' is a correct model identifier listed on 'https://huggingface.co/models'\n\n- or 'gen_lang_models/ESFP_lang_model/' is the correct path to a directory containing relevant tokenizer files\n\n"
     ]
    }
   ],
   "source": [
    "gen_model = LanguageGenerationModel(\"gpt2\", \"gen_lang_models/ESFP_lang_model/\", args={\"max_length\": 64}, use_cuda=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    'Hello, my name is',\n",
    "    'I really do not',\n",
    "    'My favorite thing to do',\n",
    "    'I really like',\n",
    "    \"I hope you don't\",\n",
    "    'You are the very reason why'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prompt in prompts:\n",
    "    # Generate text using the model. Verbose set to False to prevent logging generated sequences.\n",
    "    generated = gen_model.generate(prompt, verbose=False, args={\"max_length\": 32})\n",
    "\n",
    "    generated = '.'.join(generated[0].split('.')[:-1]) + '.'\n",
    "    print(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
