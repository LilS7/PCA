{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/linkfree_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>linkfree_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im finding lack post alarming|||sex boring pos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                  linkfree_combined\n",
       "0  INFJ  enfp intj moment sportscenter top ten play pra...\n",
       "1  ENTP  im finding lack post alarming|||sex boring pos..."
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_countvec(post):\n",
    "    return post.replace('|||', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "X = df.linkfree_combined.apply(tokenize_for_countvec)\n",
    "y = df.type\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=822)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6071,), (6071, 40948))"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect_arr = X_train_vect.toarray()\n",
    "X_test_vect_arr = X_test_vect.toarray()\n",
    "X_train_vect_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Austin/.pyenv/versions/3.7.7/envs/lewagon/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(max_iter=1000)"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_vect_arr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(log_model, open('models/not_converged_basic_log_model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6050710718401844\n"
     ]
    }
   ],
   "source": [
    "results = log_model.score(X_test_vect_arr, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test = ['hello blah blah blah', 'i love noodles', 'noodles are me', 'i think the world is a beautiful place', 'sometimes i like to go shopping', 'the things i do for love', 'man i love that movie', 'sometimes the world is a beautiful place but somtimes i hate it so much', 'wow that waas such a good movie, deadpool was amazing. highly recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test = ['jeez she def knows how to sound as bitchy as possible','when can you finally never talk to her ever again?','sounds very natural to me','what a cunt','looking forward to that day for you','finally managed to get rid of my slicey driver','can you come over real quick',\"need help grabbing mel's bag\",'she gonna meet us there','cool cool','hey! yeah i let you know a week or so ago that i was gonna miss today','just a small family trip. be back in class monday',\"i'll watch the lecture vid and work on the challanges myself\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(post_split, get_youtube=False, add_description=False):\n",
    "    #replace youtube links with youtube title\n",
    "    #return list of 50 posts\n",
    "    if get_youtube:\n",
    "        post_split = replace_youtube(post_split, add_description)\n",
    "    \n",
    "    #removes any 'words' that have http:// or https:// in them\n",
    "    #returns a list of posts if they are not empty after removing the links\n",
    "    #return list of <= 50 posts\n",
    "    post_split = remove_links(post_split)\n",
    "    \n",
    "    remove_punc = string.punctuation + '►•'\n",
    "    #remove punc and lower\n",
    "    for punctuation in remove_punc:\n",
    "        for i, item in enumerate(post_split):\n",
    "            post_split[i] = item.replace(punctuation, '').lower()\n",
    "            \n",
    "    #remove soft hyphens       \n",
    "    for i, item in enumerate(post_split):\n",
    "        post_split[i] = item.replace('\\xad', '').lower()\n",
    "        \n",
    "    #remove numbers\n",
    "    for i, item in enumerate(post_split):\n",
    "        post_split[i] = ''.join(word for word in item if not word.isdigit())\n",
    "    \n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = [word_tokenize(item) for item in post_split]\n",
    "    post_split = [[word for word in sentence if word not in stop_words] for sentence in word_tokens]\n",
    "    \n",
    "    #lemmatize if not empty sentence\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    post_split_split = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in post_split if sentence]\n",
    "    \n",
    "    #combine text into one string where each word is seperated by a space and each sentence separated by |||\n",
    "    return combine_text(post_split_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(post_split):\n",
    "    # split each post into a list of individual words\n",
    "    post_split_split = [x.split(' ') for x in post_split]\n",
    "    \n",
    "    # removes any 'words' that have http:// or https:// in them\n",
    "    return_list = [[item for item in sentence if ('http://' not in item and 'https://' not in item)] for sentence in post_split_split]\n",
    "    \n",
    "    # returns a list of posts if they are not empty after removing the links\n",
    "    return [' '.join(sentence) for sentence in return_list if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(post_split_split):\n",
    "    #takes in a list of sentences where each sentence is a list of its words\n",
    "    #and returns the one string where each word is seperated by a space and each sentence separated by |||\n",
    "    return '|||'.join([' '.join(sentence) for sentence in post_split_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test_cleaned = preprocess(Austin_test)\n",
    "random_test_cleaned = preprocess(random_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jeez def know sound bitchy possible|||finally never talk ever|||sound natural|||cunt|||looking forward day|||finally managed get rid slicey driver|||come real quick|||need help grabbing mels bag|||gon na meet u|||cool cool|||hey yeah let know week ago gon na miss today|||small family trip back class monday|||ill watch lecture vid work challanges'"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Austin_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test_cleaned = [tokenize_for_countvec(Austin_test_cleaned)]\n",
    "random_test_cleaned = [tokenize_for_countvec(random_test_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeez def know sound bitchy possible finally never talk ever sound natural cunt looking forward day finally managed get rid slicey driver come real quick need help grabbing mels bag gon na meet u cool cool hey yeah let know week ago gon na miss today small family trip back class monday ill watch lecture vid work challanges']"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Austin_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test_vect = vectorizer.transform(Austin_test_cleaned)\n",
    "random_test_vect = vectorizer.transform(random_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15])"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.predict(Austin_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ISTP'], dtype=object)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(log_model.predict(Austin_test_vect))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_model.predict(random_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['INFP'], dtype=object)"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "le.inverse_transform(log_model.predict(random_test_vect))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
