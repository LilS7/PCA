{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/tweet_clean.csv')\n",
    "text = pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'In A Promised Land, I talk about the decisions I had to make during the first few years of my presidency. Here are some thoughts on how I approach tough questions: \\', \" With COVID-19 cases reaching an all-time high this week, we\\'ve got to continue to do our part to protect one another. This pandemic is far from over and your actions can help save lives. \", \\' To all of you in Georgia, today is the last day to register to vote in the upcoming runoff election. Take a few minutes right now to register to vote, and then make sure everybody you know is registered, too. \\', \\' Thanks for sharing Marjon. I hope you get something out of A Promised Land – keep up the good work! \\', \\' Glad to see A Promised Land in such good company. \\', \" If you\\'re in Georgia, make sure you and everybody you know is registered to vote by Monday, December 7. And if you turn 18 by January 5, you\\'re eligible to vote in the runoff. Get registered right now: \", \\' Join me for a conversation with the next U.S. Senators from Georgia, @ReverendWarnock and @Ossoff, along with @StaceyAbrams and @NikemaWilliams. The stakes couldn’t be higher for their runoffs:        \\']'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['link_free_without_emoji'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF takes in a chunk of sentences within a list, with sentences wrapped in a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" \n",
    "🥨🥨🥨\n",
    "Prezel heart\n",
    "I can’t spell prezel lol\n",
    "Didnt spend my childhood time with Disney channel\n",
    "I researched her\n",
    "And still no idea who she is\n",
    "I hope you guys stay safe. I read the email about the two new cases on campus😭😭😭\n",
    "wowwwww\n",
    "Its like the Charlie’s chocolate factory invitation\n",
    "So cool\n",
    "ohhhhhh\n",
    "Hahahahahahahahahahahahahahahahaha\n",
    "Wait, I thought Davis is the UWC founder Davis\n",
    "Hes related to New Balance?\n",
    "wait\n",
    "I always think the Davis library is donated by UWC davis\n",
    "New Balance Davis is our alum??\n",
    "Oh wow\n",
    "But I’m not going to boycott Davis Library\n",
    "Lol\n",
    "hahahahahahahaha\n",
    "Can you explain the story for me please\n",
    "We are designing a product that helps analyze people’s personality based on their chatting history\n",
    "And advise how to chat to better the relationship\n",
    "hahahahahahaha\n",
    "And after we have the product, you guys should try\n",
    "lol\n",
    "Just for fun\n",
    "And it’s English\n",
    "To see what mbti of the person you are chatting with\n",
    "You upload the chat history to our prodcut\n",
    "also\n",
    "If you provide his or her phone number, we could scrape their social media account to tell the MBTI as well\n",
    "We only got two weeks\n",
    "I think we are going to have the basic model and then fine tuning it\n",
    "If we got extra time\n",
    "We are gonna fix this problem\n",
    "We got four weeks left\n",
    "But we havnet finished the lecturing\n",
    "Im not enrolled in the fall, so I’m not encluded in the Jterm\n",
    "And we met a networking event\n",
    "We didnt talk too much\n",
    "But im asking if he wanna out for a coffee chat?\n",
    "Like im using this coffee chat as an excuse\n",
    "So we can meet offline\n",
    "I don’t wanna chat online\n",
    "This is the whole story: I met him on a networking event. We’ve talked a little bit but not a lot.\n",
    "And I want to meet him in person\n",
    "Like im intereted in what he does and also him\n",
    "He said that he’s usually in office, and can’t really come out, but we could find a time to call\n",
    "But I don’t want to call him\n",
    "I want to meet him\n",
    "update: I said yes to his call\n",
    "Because he’s actually think I want to network with him\n",
    "My parents want me to get prepared\n",
    "They want me to go to grad school\n",
    "Ohhh so you are not planning to go to grad school right after college\n",
    "My parents want me to go right after\n",
    "But I don’t\n",
    "Want to\n",
    "no\n",
    "https://t.co/5pSRmD2rLF\n",
    "I truly don’t think I got the talent in physics\n",
    "And I don’t want to be a researcher in the lab\n",
    "I hate labs\n",
    "That’s the problem\n",
    "I don’t know\n",
    "But my parents think the name is important\n",
    "And also if I want to come back to China to work\n",
    "I have to go to a famous college for grad school\n",
    "Firms in china prefer those have a master degree\n",
    "Im SOOOOOOOO BADDDDD at standardized testing\n",
    "What did you get at the sat?\n",
    "I took me four times to get 1500\n",
    "Happy thanksgiving!!!!\n",
    "Hope to see you guys this coming fall\n",
    "I actually highly recommend the data science class\n",
    "It looks really practical\n",
    "A really nice touch on the resume I will say\n",
    "However, I don’t know how much you guys can learn in one month though\n",
    "But just saying, machine learning isn’t hard and does not require much coding\n",
    "Same thing for deep learning\n",
    "The hardest part is cleaning the data, which requires a lot of coding\n",
    "Like A LOT\n",
    "But im pretty sure you guys will learn machine learning\n",
    "Without machine learning, it is not data science anymore\n",
    "It is\n",
    "But good for your future\n",
    "I kind of want to audit the machine learning in the spring\n",
    "😭\n",
    "But just audit?\n",
    "Oh my\n",
    "I follow his youruve\n",
    "And he got like millions of followers\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_1 = '''Brenda when do you wanna start drinking 🍻 \n",
    "Like 10?\n",
    "And cunthia the ice is in the freezer in a cup \n",
    "Oh... 😹 idk because I think patty and Christian are sleeping ...? \n",
    "Is that fine?\n",
    "Should I go now \n",
    "I have bad service outside \n",
    "Brendaaaa \n",
    "We have the ice \n",
    "Now?\n",
    "For real \n",
    "Where are we going \n",
    "Oh omg \n",
    "Brunch 12 and not 10:30\n",
    "I was like img she wants to get brunch so early \n",
    "Bro I just woke up can we meet 12:15\n",
    "Proc? \n",
    "Take your time I still need to get dressed 😄\n",
    "LOL just come to the suite 😹\n",
    "Oh wiat can you get in\n",
    "Let me hurry up\n",
    "Is anyone doing later today\n",
    "LOL I forgot to include hw\n",
    "Yeah we’re in Monroe \n",
    "Kk and yeah we saw ! \n",
    "Yesterday my friend went to a party where there were like 60 people ...\n",
    "Yes \n",
    "Cynthia you’re in the suite right \n",
    "Please tell me you are \n",
    "She was really famous like in 2010 or something 😹😹\n",
    "My professor said soon .... uhhh ...\n",
    "I AM PREGNANT\n",
    "WITH TRIPLETS\n",
    "THE AUDACITY THIS MAN HAS\n",
    "omg i only meant to send that to BRENDA lolol omg no\n",
    "I KNOW LIKE OML 🤤🤤🤤\n",
    "but also stay in your lane sis 🙄\n",
    "aka far away from thirsting over JK\n",
    "MISS MA’AM IF YOU DONT STOP\n",
    "Cynthia are you awake ...?\n",
    "No Brenda I was asleep 🙄 since 1\n",
    "Yeah it was perfect! Sorry hehehe \n",
    "Does anyone wanna get lunch with me at 12:30 lmk\n",
    "Brenda I’m gonna start packing tonight, when can I pick up the alcohol and shot glass I have in your room 🧐\n",
    "probably will stop by tonight around 8 or 9 if that works\n",
    "Cynthia can I use the tape \n",
    "Are you in your room? \n",
    "K thanks !\n",
    "Did you just take the picture from\n",
    "The meme ? \n",
    "I’m confused \n",
    "But I think she just used the picture from Elisa Rodriguez ? \n",
    "Not y’all believing her lol\n",
    "LOL\n",
    "That’s embarrassing luv \n",
    "Can never trick a Scorpio \n",
    "Anyone wanna go to the grille \n",
    "Whenever ... \n",
    "Okay🥰\n",
    "Ready whenever you are 🥰\n",
    "Lunch  anyone ?\n",
    "When do you wanna go\n",
    "Coking \n",
    "Coming \n",
    "Cynthia do you know how to make hot chocolate from a chocolate bar \n",
    "Just one re  go all sized one ? \n",
    "Milk chocolate ? \n",
    "And we should make it in the proc basement kitchen \n",
    "What should we get cunthia ! \n",
    "We are at Midd x \n",
    "How much though \n",
    "We’ll meet you in giff basement kitchen in like 5 minutes ? Can you bring a pot...? I packed mine 😁\n",
    "We are here now 😁\n",
    "Cynthia when will you be awake tomorrow so I can give you my things\n",
    "Perfect ! \n",
    "Sweet dreams \n",
    "Don’t forget to steam life goes on 😁\n",
    "I was saying sweet dreams to you \n",
    "How could I sleep with BTS COMEBACK\n",
    "Yeah that would never be me \n",
    "Listen to it! Very healing and beautiful 🥺\n",
    "14 MINUTES\n",
    "NOT HOURS\n",
    "Lol ? \n",
    "😭\n",
    "Thank you \n",
    "I know \n",
    "🥺\n",
    "🥰\n",
    "It’s so beautiful 🥺\n",
    "And comforting \n",
    "I love them \n",
    "I’m sure no one noticed \n",
    "POV YOURE MY FAN AND YOU SEE MY AT THE AIRPORT\n",
    "BUT YOURE TOO INTIMIDATED BY MY STAR POWER TO APPROACH ME AND ASK FOR A PICTURE \n",
    "*ME\n",
    "It’s from that angle because my fans are beneath me \n",
    "Beneath me as in I’m better and superior ... not short \n",
    "The guy next to me is so cute \n",
    "Also please don’t forget Cooky 🥰🥰\n",
    "Okay uwuwuwuww thank you \n",
    "Goodbye forever 🥰🥰'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "deEmojify = deEmojify(text).split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(post_split):\n",
    "    # split each post into a list of individual words\n",
    "    post_split_split = [x.split(' ') for x in post_split]\n",
    "    \n",
    "    # removes any 'words' that have http:// or https:// in them\n",
    "    return_list = [[item for item in sentence if ('http://' not in item and 'https://' not in item)] for sentence in post_split_split]\n",
    "    \n",
    "    # returns a list of posts if they are not empty after removing the links\n",
    "    return [' '.join(sentence) for sentence in return_list if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = remove_links(deEmojify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_count(post):\n",
    "    return len(post)\n",
    "\n",
    "def word_count(post):\n",
    "    count = 0\n",
    "    for sentence in post:\n",
    "        count += len(sentence.split())\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "642"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_count(text)\n",
    "word_count(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_word_length(posts_split):\n",
    "    word_length_count = []\n",
    "    count = 0\n",
    "    for post in posts_split:\n",
    "        count += len(post)  \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3203"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_word_length(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final PreProcess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deEmojify the text\n",
    "import re\n",
    "\n",
    "def deEmojify(text):\n",
    "    regrex_pattern = re.compile(pattern = \"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags = re.UNICODE)\n",
    "    return regrex_pattern.sub(r'',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(post_split):\n",
    "    # split each post into a list of individual words\n",
    "    post_split_split = [x.split(' ') for x in post_split]\n",
    "    \n",
    "    # removes any 'words' that have http:// or https:// in them\n",
    "    return_list = [[item for item in sentence if ('http://' not in item and 'https://' not in item)] for sentence in post_split_split]\n",
    "    \n",
    "    # returns a list of posts if they are not empty after removing the links\n",
    "    return [' '.join(sentence) for sentence in return_list if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import syllables\n",
    "\n",
    "def sentence_count(post):\n",
    "    return len(post)\n",
    "\n",
    "def word_count(post):\n",
    "    count = 0\n",
    "    for sentence in post:\n",
    "        count += len(sentence.split())\n",
    "    return count\n",
    "\n",
    "def syll_count(X):\n",
    "    count_syll = 0\n",
    "    syll_count_ls = []\n",
    "    for sentence in X:\n",
    "        sentence = sentence.split()\n",
    "        for word in sentence:\n",
    "            count_syll +=  syllables.estimate(word)\n",
    "        syll_count_ls.append(count_syll)\n",
    "    return count_syll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_length_count(posts_split):\n",
    "    word_length_count = []\n",
    "    count = 0\n",
    "    for post in posts_split:\n",
    "        count += len(post)  \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_unique_word(posts):\n",
    "    count_unique = len(Counter(posts))\n",
    "    return count_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "\n",
    "def VAD(posts):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    return analyzer.polarity_scores(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "\n",
    "def NRC_emotion(posts):\n",
    "    emotion = NRCLex(posts)\n",
    "    return emotion.affect_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "word_count_scaler = pickle.load(open('../notebooks/models/word_count_scaler', 'rb'))\n",
    "unique_word_scaler = pickle.load(open('../notebooks/models/unique_word_scaler', 'rb'))\n",
    "word_length_scaler = pickle.load(open('../notebooks/models/word_length_scaler', 'rb'))\n",
    "sentence_count_scaler = pickle.load(open('../notebooks/models/sentence_count_scaler', 'rb'))   \n",
    "syll_count_scaler = pickle.load(open('../notebooks/models/syll_count_scaler', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "\n",
    "    #deEmojigy and split the text into list of sentences\n",
    "    text = deEmojify(text).split('\\n')\n",
    "    \n",
    "    #remove links\n",
    "    text = remove_links(text)\n",
    "    \n",
    "    #word, sentence,etc. count and scaled\n",
    "    word_count_scaled = word_count_scaler.transform(np.array(word_count(text)).reshape(-1,1))\n",
    "    count_unique_word_scaled = unique_word_scaler.transform(np.array(count_unique_word(text)).reshape(-1,1))\n",
    "    word_length_count_scaled = word_length_scaler.transform(np.array(word_length_count(text)).reshape(-1,1))\n",
    "    sentence_count_scaled = sentence_count_scaler.transform(np.array(sentence_count(text)).reshape(-1,1))\n",
    "    syll_count_scaled = syll_count_scaler.transform(np.array(syll_count(text)).reshape(-1,1)) \n",
    "\n",
    "    \n",
    "    #word_per_sentence\n",
    "    avg_word_length = word_length_count_scaled/word_count_scaled\n",
    "    word_per_sentence = word_count_scaled/sentence_count_scaled\n",
    "    \n",
    "    #TTR\n",
    "    TTR = count_unique_word_scaled/word_count_scaled\n",
    "    \n",
    "    #join all the sentences together and then VAD them\n",
    "    text_joined = ''.join(text)\n",
    "    VAD_dict = VAD(text_joined)\n",
    "    \n",
    "    #NRC emotion\n",
    "    emotion_dict = NRC_emotion(text_joined)\n",
    "    \n",
    "    #map the VAD and NRC emotion\n",
    "    neg_sentiment = VAD_dict['neg']\n",
    "    pos_sentiment = VAD_dict['pos']\n",
    "    compound_sentiment = VAD_dict['compound']\n",
    "    neu_sentiment = VAD_dict['neu']\n",
    "    \n",
    "    fear = emotion_dict['fear']\n",
    "    anger = emotion_dict['anger']\n",
    "    anticip = emotion_dict['anticip']\n",
    "    trust = emotion_dict['trust']\n",
    "    surprise = emotion_dict['surprise']\n",
    "    positive = emotion_dict['positive']\n",
    "    negative = emotion_dict['negative']\n",
    "    sadness = emotion_dict['sadness']\n",
    "    disgust = emotion_dict['disgust']\n",
    "    joy = emotion_dict['joy']\n",
    "    \n",
    "    input_ls = np.array([neg_sentiment,\n",
    "                neu_sentiment,\n",
    "                pos_sentiment,\n",
    "                compound_sentiment,\n",
    "                fear,\n",
    "                anger,\n",
    "                anticip,\n",
    "                trust,\n",
    "                surprise,\n",
    "                positive,\n",
    "                negative,\n",
    "                sadness,\n",
    "                disgust,\n",
    "                joy,\n",
    "                float(syll_count_scaled),\n",
    "                float(word_count_scaled),\n",
    "                float(count_unique_word_scaled),\n",
    "                float(word_length_count_scaled),\n",
    "                float(sentence_count_scaled),\n",
    "                float(TTR),\n",
    "                float(avg_word_length),\n",
    "                float(word_per_sentence_scaled)])\n",
    "    return input_ls.reshape(-1,21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_text = preprocess(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.055     ,  0.801     ,  0.144     ,  0.9948    ,  0.07407407,\n",
       "         0.03703704,  0.        ,  0.13580247,  0.03703704,  0.2345679 ,\n",
       "         0.13580247,  0.07407407,  0.01234568,  0.12345679, -1.83465356,\n",
       "        -2.14948012, -2.97568242,  1.66844419, -2.97568242,  1.38437308,\n",
       "        -0.77620824]])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ei_model = pickle.load(open('../notebooks/models/RF_IE_Model_2.0', 'rb'))\n",
    "jp_model = pickle.load(open('../notebooks/models/RF_JP_Model_2.0', 'rb'))\n",
    "ns_model = pickle.load(open('../notebooks/models/RF_NS_Model_2.0', 'rb'))\n",
    "tf_model = pickle.load(open('../notebooks/models/RF_TF_Model_2.0', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ei_model.predict(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jp_model.predict(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ns_model.predict(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_model.predict(processed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
