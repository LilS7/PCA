{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/linkfree_combined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>linkfree_combined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INFJ</td>\n",
       "      <td>enfp intj moment sportscenter top ten play pra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ENTP</td>\n",
       "      <td>im finding lack post alarming|||sex boring pos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   type                                  linkfree_combined\n",
       "0  INFJ  enfp intj moment sportscenter top ten play pra...\n",
       "1  ENTP  im finding lack post alarming|||sex boring pos..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_for_countvec(post):\n",
    "    return post.replace('|||', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def four_labels(type_):\n",
    "#     analysts = ['INTJ', 'INTP', 'ENTJ', 'ENTP']\n",
    "#     diplomats = ['INFJ', 'INFP', 'ENFJ', 'ENFP']\n",
    "#     sentinels = ['ISTJ', 'ISFJ', 'ESTJ', 'ESFJ']\n",
    "#     explorers = ['ISTP', 'ISFP', 'ESTP', 'ESFP']\n",
    "    types = {'INTJ':0, 'INTP':0, 'ENTJ':0, 'ENTP':0, \n",
    "             'INFJ':1, 'INFP':1, 'ENFJ':1, 'ENFP':1, \n",
    "             'ISTJ':2, 'ISFJ':2, 'ESTJ':2, 'ESFJ':2, \n",
    "             'ISTP':3, 'ISFP':3, 'ESTP':3, 'ESFP':3}\n",
    "    \n",
    "    return types[type_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.linkfree_combined.apply(tokenize_for_countvec)\n",
    "y = df.type.apply(four_labels)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=822)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=2)\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6071,), (6071, 40948))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_train_vect.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vect_arr = X_train_vect.toarray()\n",
    "X_test_vect_arr = X_test_vect.toarray()\n",
    "X_train_vect_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier()"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_model = KNeighborsClassifier()\n",
    "knn_model.fit(X_train_vect_arr, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(knn_model, open('models/4_cat_basic_knn_model', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5601229350749135\n"
     ]
    }
   ],
   "source": [
    "results = knn_model.score(X_test_vect_arr, y_test)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test = ['hello blah blah blah', 'i love noodles', 'noodles are me', 'i think the world is a beautiful place', 'sometimes i like to go shopping', 'the things i do for love', 'man i love that movie', 'sometimes the world is a beautiful place but somtimes i hate it so much', 'wow that waas such a good movie, deadpool was amazing. highly recommended']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test = ['jeez she def knows how to sound as bitchy as possible','when can you finally never talk to her ever again?','sounds very natural to me','what a cunt','looking forward to that day for you','finally managed to get rid of my slicey driver','can you come over real quick',\"need help grabbing mel's bag\",'she gonna meet us there','cool cool','hey! yeah i let you know a week or so ago that i was gonna miss today','just a small family trip. be back in class monday',\"i'll watch the lecture vid and work on the challanges myself\", 'Feel like i would rather work at least 4 days a week just for more monies', 'I see i see. Man going to school while also working gotta be rough', 'Oh you wanna ktv at my place? That works for me', 'Hows you get out so quick?', 'Ight that makes sense. Lets do it at my place thens', 'Bowling probably soon cause bowling is over at 5', 'Haha ight i would love to see your car bro', 'Iâ€™ll text you after bowling then. Cina coming to eat pho or naw? Bouta go ask my momma', 'To see if we gots enough', 'After bowling itll be a little bit of costco then home', 'Yeah she said come thru haha', 'On the highway from rivertown costco. On M6 exit 5 just passed', 'Shes down for wtv. She said whatever you recommend', 'Ohhhh shiet yeah im so down', 'Hell yeah dude! Its a date', 'Ohhh yeah lets do that', 'That sounds good too', 'We making a res? Curtis joining too', 'I think 6 should be fine. We just ate lunch haha', 'Oh ight haha im down for red robins beo', 'Hahaha oh yeah baby thats the america i know! Im hype for red robins haha', 'Just finished packing up mostly. Now gonna head to woodland to walk around a bit', 'Thats the plan', 'Ight haha so you out from work already? Lol shiet thats chill', 'Yessir', 'Thanks bro ðŸ˜­ðŸ˜­ðŸ˜­ dont wanna leave haha', 'Next time we moving to gr ;) buy me a house', 'Ill be your butler']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shirley_test = ['ðŸ¥¨ðŸ¥¨ðŸ¥¨', 'Prezel heart', 'I canâ€™t spell prezel lol', 'Didnt spend my childhood time with Disney channel', 'I researched her', 'And still no idea who she is', 'I hope you guys stay safe. I read the email about the two new cases on campusðŸ˜­ðŸ˜­ðŸ˜­', 'wowwwww', 'Its like the Charlieâ€™s chocolate factory invitation', 'So cool', 'ohhhhhh', 'Hahahahahahahahahahahahahahahahaha', 'Wait, I thought Davis is the UWC founder Davis', 'Hes related to New Balance?', 'wait', 'I always think the Davis library is donated by UWC davis', 'New Balance Davis is our alum??', 'Oh wow', 'But Iâ€™m not going to boycott Davis Library', 'Lol', 'hahahahahahahaha', 'Can you explain the story for me please', 'We are designing a product that helps analyze peopleâ€™s personality based on their chatting history', 'And advise how to chat to better the relationship', 'hahahahahahaha', 'And after we have the product, you guys should try', 'lol', 'Just for fun', 'And itâ€™s English', 'To see what mbti of the person you are chatting with', 'You upload the chat history to our prodcut', 'also', 'If you provide his or her phone number, we could scrape their social media account to tell the MBTI as well', 'We only got two weeks', 'I think we are going to have the basic model and then fine tuning it', 'If we got extra time', 'We are gonna fix this problem', 'We got four weeks left', 'But we havnet finished the lecturing', 'Im not enrolled in the fall, so Iâ€™m not encluded in the Jterm', 'And we met a networking event', 'We didnt talk too much', 'But im asking if he wanna out for a coffee chat?', 'Like im using this coffee chat as an excuse', 'So we can meet offline', 'I donâ€™t wanna chat online', 'This is the whole story: I met him on a networking event. Weâ€™ve talked a little bit but not a lot.', 'And I want to meet him in person', 'Like im intereted in what he does and also him', 'He said that heâ€™s usually in office, and canâ€™t really come out, but we could find a time to call', 'But I donâ€™t want to call him', 'I want to meet him', 'update: I said yes to his call', 'Because heâ€™s actually think I want to network with him', 'My parents want me to get prepared', 'They want me to go to grad school', 'Ohhh so you are not planning to go to grad school right after college', 'My parents want me to go right after', 'But I donâ€™t', 'Want to', 'no', 'I truly donâ€™t think I got the talent in physics', 'And I donâ€™t want to be a researcher in the lab', 'I hate labs', 'Thatâ€™s the problem', 'I donâ€™t know', 'But my parents think the name is important', 'And also if I want to come back to China to work', 'I have to go to a famous college for grad school', 'Firms in china prefer those have a master degree', 'Im SOOOOOOOO BADDDDD at standardized testing', 'What did you get at the sat?', 'I took me four times to get 1500', 'Happy thanksgiving!!!!', 'Hope to see you guys this coming fall', 'I actually highly recommend the data science class', 'It looks really practical', 'A really nice touch on the resume I will say', 'However, I donâ€™t know how much you guys can learn in one month though', 'But just saying, machine learning isnâ€™t hard and does not require much coding', 'Same thing for deep learning', 'The hardest part is cleaning the data, which requires a lot of coding', 'Like A LOT', 'But im pretty sure you guys will learn machine learning', 'Without machine learning, it is not data science anymore', 'It is', 'But good for your future', 'I kind of want to audit the machine learning in the spring', 'ðŸ˜­', 'But just audit?', 'Oh my', 'I follow his youruve', 'And he got like millions of followers']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(post_split, get_youtube=False, add_description=False):\n",
    "    #replace youtube links with youtube title\n",
    "    #return list of 50 posts\n",
    "    if get_youtube:\n",
    "        post_split = replace_youtube(post_split, add_description)\n",
    "    \n",
    "    #removes any 'words' that have http:// or https:// in them\n",
    "    #returns a list of posts if they are not empty after removing the links\n",
    "    #return list of <= 50 posts\n",
    "    post_split = remove_links(post_split)\n",
    "    \n",
    "    remove_punc = string.punctuation + 'â–ºâ€¢'\n",
    "    #remove punc and lower\n",
    "    for punctuation in remove_punc:\n",
    "        for i, item in enumerate(post_split):\n",
    "            post_split[i] = item.replace(punctuation, '').lower()\n",
    "            \n",
    "    #remove soft hyphens       \n",
    "    for i, item in enumerate(post_split):\n",
    "        post_split[i] = item.replace('\\xad', '').lower()\n",
    "        \n",
    "    #remove numbers\n",
    "    for i, item in enumerate(post_split):\n",
    "        post_split[i] = ''.join(word for word in item if not word.isdigit())\n",
    "    \n",
    "    #remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = [word_tokenize(item) for item in post_split]\n",
    "    post_split = [[word for word in sentence if word not in stop_words] for sentence in word_tokens]\n",
    "    \n",
    "    #lemmatize if not empty sentence\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    post_split_split = [[lemmatizer.lemmatize(word) for word in sentence] for sentence in post_split if sentence]\n",
    "    \n",
    "    #combine text into one string where each word is seperated by a space and each sentence separated by |||\n",
    "    return combine_text(post_split_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(post_split):\n",
    "    # split each post into a list of individual words\n",
    "    post_split_split = [x.split(' ') for x in post_split]\n",
    "    \n",
    "    # removes any 'words' that have http:// or https:// in them\n",
    "    return_list = [[item for item in sentence if ('http://' not in item and 'https://' not in item)] for sentence in post_split_split]\n",
    "    \n",
    "    # returns a list of posts if they are not empty after removing the links\n",
    "    return [' '.join(sentence) for sentence in return_list if sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_text(post_split_split):\n",
    "    #takes in a list of sentences where each sentence is a list of its words\n",
    "    #and returns the one string where each word is seperated by a space and each sentence separated by |||\n",
    "    return '|||'.join([' '.join(sentence) for sentence in post_split_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test_cleaned = preprocess(Austin_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shirley_test_cleaned = preprocess(Shirley_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_cleaned = preprocess(random_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'jeez def know sound bitchy possible|||finally never talk ever|||sound natural|||cunt|||looking forward day|||finally managed get rid slicey driver|||come real quick|||need help grabbing mels bag|||gon na meet u|||cool cool|||hey yeah let know week ago gon na miss today|||small family trip back class monday|||ill watch lecture vid work challanges|||feel like would rather work least day week monies|||see see man going school also working got ta rough|||oh wan na ktv place work|||hows get quick|||ight make sense let place then|||bowling probably soon cause bowling|||haha ight would love see car bro|||â€™ text bowling cina coming eat pho naw bouta go ask momma|||see gots enough|||bowling itll little bit costco home|||yeah said come thru haha|||highway rivertown costco exit passed|||shes wtv said whatever recommend|||ohhhh shiet yeah im|||hell yeah dude date|||ohhh yeah let|||sound good|||making re curtis joining|||think fine ate lunch haha|||oh ight haha im red robin beo|||hahaha oh yeah baby thats america know im hype red robin haha|||finished packing mostly gon na head woodland walk around bit|||thats plan|||ight haha work already lol shiet thats chill|||yessir|||thanks bro ðŸ˜­ðŸ˜­ðŸ˜­ dont wan na leave haha|||next time moving gr buy house|||ill butler'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Austin_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test_cleaned = [tokenize_for_countvec(Austin_test_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shirley_test_cleaned = [tokenize_for_countvec(Shirley_test_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_cleaned = [tokenize_for_countvec(random_test_cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['jeez def know sound bitchy possible finally never talk ever sound natural cunt looking forward day finally managed get rid slicey driver come real quick need help grabbing mels bag gon na meet u cool cool hey yeah let know week ago gon na miss today small family trip back class monday ill watch lecture vid work challanges feel like would rather work least day week monies see see man going school also working got ta rough oh wan na ktv place work hows get quick ight make sense let place then bowling probably soon cause bowling haha ight would love see car bro â€™ text bowling cina coming eat pho naw bouta go ask momma see gots enough bowling itll little bit costco home yeah said come thru haha highway rivertown costco exit passed shes wtv said whatever recommend ohhhh shiet yeah im hell yeah dude date ohhh yeah let sound good making re curtis joining think fine ate lunch haha oh ight haha im red robin beo hahaha oh yeah baby thats america know im hype red robin haha finished packing mostly gon na head woodland walk around bit thats plan ight haha work already lol shiet thats chill yessir thanks bro ðŸ˜­ðŸ˜­ðŸ˜­ dont wan na leave haha next time moving gr buy house ill butler']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Austin_test_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "Austin_test_vect = vectorizer.transform(Austin_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Shirley_test_vect = vectorizer.transform(Shirley_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_test_vect = vectorizer.transform(random_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "kd_tree does not work with sparse matrices. Densify the data, or set algorithm='brute'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-3b3c504512e0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mknn_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAustin_test_vect\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\admin\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\neighbors\\_classification.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    173\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 175\u001b[1;33m         \u001b[0mneigh_dist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mneigh_ind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkneighbors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    176\u001b[0m         \u001b[0mclasses_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[0m_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_y\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\admin\\.venvs\\lewagon\\lib\\site-packages\\sklearn\\neighbors\\_base.py\u001b[0m in \u001b[0;36mkneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    648\u001b[0m                 raise ValueError(\n\u001b[0;32m    649\u001b[0m                     \u001b[1;34m\"%s does not work with sparse matrices. Densify the data, \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 650\u001b[1;33m                     \"or set algorithm='brute'\" % self._fit_method)\n\u001b[0m\u001b[0;32m    651\u001b[0m             old_joblib = (\n\u001b[0;32m    652\u001b[0m                     parse_version(joblib.__version__) < parse_version('0.12'))\n",
      "\u001b[1;31mValueError\u001b[0m: kd_tree does not work with sparse matrices. Densify the data, or set algorithm='brute'"
     ]
    }
   ],
   "source": [
    "knn_model.predict(Austin_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.predict(random_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model.predict(Shirley_test_vect)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
